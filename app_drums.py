import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import streamlit as st
import numpy as np
import librosa
import librosa.display
import soundfile as sf
import io
import tempfile
import os
import pandas as pd
import json
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import plotly.graph_objects as go
import scipy.signal

# Import our backend modules
from division import AudioRemixer
# from drum_processor import DrumLoopExtractor # No longer needed for slicer only

st.set_page_config(layout="wide", page_title="LoopHunter - BPM Slicer")

st.markdown("""
<style>
    .main { background-color: #0d1117; }
    .stButton>button { width: 100%; border-radius: 6px; font-weight: 600; }
    h1, h2, h3, p, label, .stMetricLabel { color: #c9d1d9 !important; }
    .stMetricValue { color: #3b82f6 !important; }
    .stDownloadButton button { height: 3rem; }
</style>
""", unsafe_allow_html=True)

st.title("âœ‚ï¸ LoopHunter - BPM Slicer")
st.caption("AI-powered tool for BPM-based audio slicing.")

if 'remixer' not in st.session_state: st.session_state.remixer = None
if 'beat_slices' not in st.session_state: st.session_state.beat_slices = None
if 'bpm_info' not in st.session_state: st.session_state.bpm_info = None

def estimate_bpm_from_times(times_sec):
    """ç¨³å¥ BPM ä¼°è®¡ï¼šåŸºäºåˆ‡ç‚¹é—´éš”çš„ä¸­ä½æ•°ï¼ˆæ¯”å‡å€¼æ›´æŠ—æ¼æ‹/å™ªå£°ï¼‰ã€‚"""
    if times_sec is None:
        return 0.0
    t = np.array(times_sec, dtype=float)
    t = t[np.isfinite(t)]
    if t.size < 2:
        return 0.0
    t = np.sort(np.unique(t))
    if t.size < 2:
        return 0.0
    diffs = np.diff(t)
    # è¿‡æ»¤å¼‚å¸¸é—´éš”ï¼ˆè¿‡å°é€šå¸¸æ˜¯é‡å¤ç‚¹/å™ªå£°ï¼›è¿‡å¤§é€šå¸¸æ˜¯æ¼æ‹/å°¾å¥ï¼‰
    diffs = diffs[(diffs > 0.12) & (diffs < 2.0)]
    if diffs.size == 0:
        return 0.0
    return float(60.0 / np.median(diffs))

def estimate_bpm_librosa(y, sr):
    """librosa tempo ä¼°è®¡ï¼ˆæ•´ä½“èŠ‚å¥ï¼‰ï¼Œå¯¹å¼±æ‹/æ¼æ‹é€šå¸¸æ›´ç¨³ã€‚"""
    try:
        onset_env = librosa.onset.onset_strength(y=y, sr=sr)
        tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)
        if tempo is None or len(tempo) == 0:
            return 0.0
        return float(tempo[0])
    except Exception:
        return 0.0

def estimate_bpm_best(y, sr, bpm_min=75.0, bpm_max=200.0, hop_length=512):
    """
    BPM ä¼°è®¡ï¼šä½¿ç”¨ JMPerez/beats-audio-api çš„ç®—æ³•
    (åŸºäº 100-150Hz ä½é¢‘èƒ½é‡å³°å€¼æ£€æµ‹ä¸é—´éš”ç»Ÿè®¡)
    
    å‚æ•°:
    - bpm_min: BPM ä¸‹é™ (é»˜è®¤ 75.0ï¼Œä»¥é¿å… 134 BPM è¢«è¯¯åˆ¤ä¸º 67)
    - bpm_max: BPM ä¸Šé™ (é»˜è®¤ 200.0)
    """
    try:
        # 1. æ»¤æ³¢ï¼šBandpass 100-150Hz (Lowpass 150 + Highpass 100)
        # Web Audio API é»˜è®¤ Biquad æ˜¯ 12dB/oct (2nd order)
        sos_lp = scipy.signal.butter(2, 150, 'low', fs=sr, output='sos')
        y_lp = scipy.signal.sosfilt(sos_lp, y)
        
        sos_hp = scipy.signal.butter(2, 100, 'high', fs=sr, output='sos')
        y_filt = scipy.signal.sosfilt(sos_hp, y_lp)
        
        # 2. å³°å€¼æ£€æµ‹ (Get Peaks)
        # å°†éŸ³é¢‘åˆ†ä¸º 0.5s çš„ç‰‡æ®µï¼Œæ‰¾æ¯æ®µæœ€å¤§å€¼
        part_size = int(sr * 0.5)
        if part_size == 0: return {"bpm": 0.0, "confidence": 0.0, "base_bpm": 0.0, "candidates": []}
        
        parts = len(y_filt) // part_size
        peaks = []
        
        for i in range(parts):
            start = i * part_size
            end = start + part_size
            chunk = y_filt[start:end]
            
            if len(chunk) == 0: continue
            
            # æ‰¾æœ€å¤§æŒ¯å¹…
            max_idx = np.argmax(np.abs(chunk))
            max_vol = float(np.abs(chunk[max_idx]))
            
            if max_vol > 0:
                peaks.append({
                    'position': start + max_idx,
                    'volume': max_vol
                })
        
        if not peaks:
             return {"bpm": 0.0, "confidence": 0.0, "base_bpm": 0.0, "candidates": []}

        # æŒ‰éŸ³é‡é™åº
        peaks.sort(key=lambda x: x['volume'], reverse=True)
        
        # å–å‰ 50% æœ€å“çš„
        take_count = max(1, len(peaks) // 2)
        peaks = peaks[:take_count]
        
        # æŒ‰ä½ç½®(æ—¶é—´)é‡æ–°æ’åº
        peaks.sort(key=lambda x: x['position'])
        
        # 3. é—´éš”ç»Ÿè®¡ (Get Intervals)
        groups = []
        
        for index, peak in enumerate(peaks):
            # å¯¹æ¯”æ¥ä¸‹æ¥çš„ 10 ä¸ªå³°å€¼
            for i in range(1, 10):
                if index + i >= len(peaks):
                    break
                
                neighbor = peaks[index + i]
                diff_samples = neighbor['position'] - peak['position']
                if diff_samples <= 0: continue
                
                tempo = (60.0 * sr) / diff_samples
                
                # JMPerez é€»è¾‘ï¼šå½’ä¸€åŒ–åˆ°æŒ‡å®šèŒƒå›´ (é»˜è®¤ 75-200)
                # å¦‚æœ bpm_min/bpm_max å‚æ•°æœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨ 75/200 é»˜è®¤å€¼
                # åŸç®—æ³•æ˜¯ 90-180ï¼Œè¿™é‡Œæ”¾å®½ä»¥æ”¯æŒ 80 BPMï¼Œå¹¶å°†ä¸‹é™è®¾ä¸º 75 ä»¥é¿å… 134 è¢«è¯¯åˆ¤ä¸º 67
                min_limit = bpm_min if bpm_min > 0 else 75.0
                max_limit = bpm_max if bpm_max > 0 else 200.0
                
                while tempo < min_limit:
                    tempo *= 2
                while tempo > max_limit:
                    tempo /= 2
                    
                tempo = round(tempo)
                
                # ç»Ÿè®¡
                found = False
                for g in groups:
                    if g['tempo'] == tempo:
                        g['count'] += 1
                        found = True
                        break
                if not found:
                    groups.append({'tempo': tempo, 'count': 1})
        
        if not groups:
            return {"bpm": 0.0, "confidence": 0.0, "base_bpm": 0.0, "candidates": []}

        # æŒ‰ count é™åº
        groups.sort(key=lambda x: x['count'], reverse=True)
        
        best_group = groups[0]
        best_bpm = float(best_group['tempo'])
        best_count = best_group['count']
        
        # ç®€å•è®¡ç®—ç½®ä¿¡åº¦
        total_count = sum(g['count'] for g in groups)
        confidence = float(best_count) / total_count if total_count > 0 else 0.0
        
        # æ„é€  candidates æ ¼å¼
        candidates = [(float(g['tempo']), float(g['count'])) for g in groups[:5]]
        
        return {
            "bpm": best_bpm,
            "confidence": confidence,
            "base_bpm": best_bpm,
            "candidates": candidates
        }

    except Exception:
        return {"bpm": 0.0, "confidence": 0.0, "base_bpm": 0.0, "candidates": []}

def detect_active_bounds(y, sr, top_db=45):
    """
    ä¼°è®¡â€œæœ‰æ•ˆå†…å®¹â€çš„èµ·æ­¢ï¼ˆå»æ‰å¼€å¤´/ç»“å°¾çš„é™éŸ³æˆ–ä½ç”µå¹³åº•å™ªï¼‰ã€‚
    è¿”å› (start_time, end_time) ç§’ã€‚
    """
    try:
        intervals = librosa.effects.split(y, top_db=top_db)
        if intervals is None or len(intervals) == 0:
            return 0.0, float(len(y) / sr)
        start_samp = int(intervals[0][0])
        end_samp = int(intervals[-1][1])
        start_t = float(start_samp) / sr
        end_t = float(end_samp) / sr
        # é˜²å¾¡ï¼šè‡³å°‘ä¿è¯æœ‰æ­£é•¿åº¦
        if end_t <= start_t:
            return 0.0, float(len(y) / sr)
        return start_t, min(end_t, float(len(y) / sr))
    except Exception:
        return 0.0, float(len(y) / sr)

def refine_time_to_zero_crossing(y, sr, t_sec, window_ms=8):
    """æŠŠåˆ‡ç‚¹å¾®è°ƒåˆ°é™„è¿‘è¿‡é›¶ç‚¹/ä½å¹…åº¦ç‚¹ï¼Œå‡å°‘åˆ‡å‰²çˆ†éŸ³ã€‚"""
    n = len(y)
    if n == 0:
        return float(t_sec)
    idx = int(round(t_sec * sr))
    idx = max(0, min(n - 1, idx))
    w = max(1, int(sr * (window_ms / 1000.0)))
    s = max(0, idx - w)
    e = min(n - 1, idx + w)
    seg = y[s:e+1]
    if seg.size < 3:
        return float(idx) / sr
    # ä¼˜å…ˆæ‰¾çœŸæ­£çš„è¿‡é›¶ç‚¹ï¼ˆç¬¦å·å˜åŒ–ï¼‰
    signs = np.sign(seg)
    zc = np.where(np.diff(np.signbit(seg)))[0]
    if zc.size > 0:
        # é€‰æ‹©ç¦»ä¸­å¿ƒæœ€è¿‘çš„è¿‡é›¶ç‚¹
        center = idx - s
        best = zc[np.argmin(np.abs(zc - center))]
        return float(s + best) / sr
    # å¦åˆ™é€‰å¹…åº¦æœ€å°ç‚¹
    best = int(np.argmin(np.abs(seg)))
    return float(s + best) / sr

def apply_short_fade(x, sr, fade_ms=5):
    """è¯•å¬ç”¨ï¼šå¯¹åˆ‡ç‰‡åšæçŸ­æ·¡å…¥æ·¡å‡ºï¼Œè¿›ä¸€æ­¥é¿å…ç‚¹å‡»éŸ³ã€‚"""
    if x is None or len(x) == 0:
        return x
    fade_len = int(sr * (fade_ms / 1000.0))
    fade_len = max(0, min(fade_len, len(x) // 2))
    if fade_len <= 0:
        return x
    x = x.copy()
    win_in = np.linspace(0.0, 1.0, fade_len, endpoint=False)
    win_out = np.linspace(1.0, 0.0, fade_len, endpoint=False)
    x[:fade_len] *= win_in
    x[-fade_len:] *= win_out
    return x

def refine_beat_times(y, sr, beat_times):
    # Calculate onset envelope
    onset_env = librosa.onset.onset_strength(y=y, sr=sr)
    # Detect onsets with backtracking for better transient precision
    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, backtrack=True)
    onset_times = librosa.frames_to_time(onset_frames, sr=sr)
    
    refined = []
    # Optimization: Sort onset_times to speed up search or just iterate (len is small)
    for t in beat_times:
        # Search window +/- 60ms
        diffs = np.abs(onset_times - t)
        if len(diffs) > 0:
            min_idx = np.argmin(diffs)
            if diffs[min_idx] < 0.06:
                refined.append(onset_times[min_idx])
            else:
                refined.append(t)
        else:
            refined.append(t)
    return np.unique(refined) # Remove potential duplicates

def get_beat_slices(y, sr, beat_times, total_duration, bpm_override=None):
    """
    æ›´â€œå¥½å¬â€çš„ BPM åˆ‡ç‰‡ï¼š
    - å…ˆç”¨èƒ½é‡åˆ†å‰²ä¼°è®¡æœ‰æ•ˆèµ·æ­¢ï¼Œé¿å…åº•å™ª/é™éŸ³å¹²æ‰°
    - ç”¨ beat_times æ¨æ–­ç¨³å®š periodï¼Œå¹¶ç”Ÿæˆæ›´è§„æ•´çš„ beat ç½‘æ ¼ï¼ˆæ›´ä¸€è‡´ï¼‰
    - æ¯ä¸ªåˆ‡ç‚¹å…ˆå¸é™„ç¬æ€ï¼Œå†å¾®è°ƒåˆ°è¿‡é›¶ç‚¹ï¼Œå‡å°‘çˆ†éŸ³/æˆªæ–­æ„Ÿ
    - é»˜è®¤ä¸¢å¼ƒ/åˆå¹¶è¿‡çŸ­çš„â€œå¼±èµ·/å°¾å·´â€åˆ‡ç‰‡ï¼ˆä»ä¿æŒæŒ‰ BPM ç½‘æ ¼ï¼‰
    """
    total_duration = float(total_duration)
    if total_duration <= 0:
        total_duration = float(len(y) / sr)

    # 0) æœ‰æ•ˆå†…å®¹èŒƒå›´ï¼ˆæ›´å‡†çš„å…¨æ›²èµ·æ­¢ï¼‰
    active_start, active_end = detect_active_bounds(y, sr, top_db=45)
    active_start = max(0.0, min(active_start, total_duration))
    active_end = max(active_start, min(active_end, total_duration))

    # 1) å…ˆåšç¬æ€å¸é™„ï¼ˆé¿å…åˆ‡åœ¨â€œåŠå±±è…°â€ï¼‰
    bt = np.array(beat_times, dtype=float) if beat_times is not None else np.array([], dtype=float)
    bt = bt[np.isfinite(bt)]
    bt = bt[(bt >= 0.0) & (bt <= total_duration)]
    bt = np.sort(np.unique(bt))

    if bt.size > 0:
        bt = refine_beat_times(y, sr, bt)
        bt = bt[(bt >= 0.0) & (bt <= total_duration)]
        bt = np.sort(np.unique(bt))

    # 2) æ¨æ–­ periodï¼ˆä¼˜å…ˆä½¿ç”¨ bpm_overrideï¼›å¦åˆ™ç”¨ beat_times/tempoï¼‰
    period = None
    if bpm_override is not None:
        try:
            b = float(bpm_override)
            if np.isfinite(b) and b > 0:
                period = float(60.0 / b)
        except Exception:
            period = None
    if bt.size >= 3:
        diffs = np.diff(bt)
        diffs = diffs[(diffs > 0.12) & (diffs < 2.0)]
        if diffs.size > 0:
            period = float(np.median(diffs))
    if period is None or period <= 0:
        tempo = estimate_bpm_librosa(y, sr)
        if tempo and tempo > 0:
            period = float(60.0 / tempo)
        else:
            # å…œåº•ï¼šå‡è®¾ 120 BPM
            period = 0.5

    # 3) ç”Ÿæˆæ›´è§„æ•´çš„ beat ç½‘æ ¼ï¼ˆè®©åˆ‡ç‰‡é•¿åº¦æ›´ä¸€è‡´ï¼‰
    grid = []
    if bt.size > 0:
        anchor = float(bt[0])
    else:
        anchor = active_start

    # è®©ç½‘æ ¼è¦†ç›–æœ‰æ•ˆå†…å®¹èŒƒå›´ï¼ˆç•¥æ‰©ä¸€ç‚¹ï¼Œé˜²æ­¢è¾¹ç•Œæ¼æ‰ï¼‰
    start_n = int(np.floor((active_start - anchor) / period)) - 1
    end_n = int(np.ceil((active_end - anchor) / period)) + 1
    for n in range(start_n, end_n + 1):
        t = anchor + n * period
        if active_start - 0.25 * period <= t <= active_end + 0.25 * period:
            grid.append(float(t))
    grid = np.sort(np.unique(np.array(grid, dtype=float)))

    # 3.5) å°è¯•å°†ç½‘æ ¼å¯¹é½åˆ°æ£€æµ‹åˆ°çš„ beat_timesï¼ˆä¿®æ­£ç´¯ç§¯æ¼‚ç§»ï¼‰
    # çº¿æ€§ç½‘æ ¼å®¹æ˜“åœ¨åé¢äº§ç”Ÿç´¯ç§¯è¯¯å·®ï¼Œå¯¼è‡´åˆ‡ç‚¹åç¦»ï¼ˆå¦‚ slice 10 ååï¼‰ã€‚
    # è¿™é‡Œåˆ©ç”¨ librosa æ£€æµ‹åˆ°çš„ beat_timesï¼ˆé€šå¸¸æ›´è´´åˆéŸ³é¢‘å˜åŒ–ï¼‰æ¥ä¿®æ­£ç½‘æ ¼ä½ç½®ã€‚
    if bt.size > 0:
        synced_grid = []
        # å…è®¸æœ€å¤§æ¼‚ç§»çª—å£ï¼šå‘¨æœŸçš„ä¸€åŠæˆ–å›ºå®šå€¼ï¼Œå–è¾ƒå°å€¼é˜²æ­¢è·³æ‹
        # 0.35 * period èƒ½å®¹å¿ä¸€å®šç¨‹åº¦çš„ tempo å˜åŒ–ï¼ŒåŒæ—¶é¿å…å¸é™„åˆ°ç›¸é‚»æ‹
        sync_window = 0.35 * period if period and period > 0 else 0.15
        
        for g in grid:
            # æ‰¾æœ€è¿‘çš„æ£€æµ‹ beat
            idx = (np.abs(bt - g)).argmin()
            nearest = bt[idx]
            dist = abs(nearest - g)
            
            # å¦‚æœåœ¨å…è®¸èŒƒå›´å†…ï¼Œè¯´æ˜æ£€æµ‹åˆ°äº†å¯¹åº”çš„ beatï¼Œä¼˜å…ˆä½¿ç”¨æ£€æµ‹å€¼ï¼ˆå› ä¸ºå®ƒå·²ç»åŒ…å«ç¬æ€å¯¹é½ï¼‰
            if dist < sync_window:
                synced_grid.append(nearest)
            else:
                synced_grid.append(g)
        
        # é‡æ–°æ’åºå¹¶å»é‡
        grid = np.array(synced_grid)
        grid = np.sort(np.unique(grid))

    # 4) æŠŠç½‘æ ¼å¸é™„åˆ°æœ€è¿‘ç¬æ€ï¼ˆå°çª—å£å†…ï¼‰ï¼Œé¿å…æœºæ¢°åˆ‡å‰²
    grid = refine_beat_times(y, sr, grid)
    grid = grid[(grid >= 0.0) & (grid <= total_duration)]
    grid = np.sort(np.unique(grid))

    # 5) æ„é€ åˆ‡ç‚¹ï¼šé»˜è®¤ä»â€œç¬¬ä¸€ä¸ªå®Œæ•´æ‹â€å¼€å§‹ï¼ˆå‡å°‘å¾ˆçŸ­ slice1ï¼‰
    cuts = []
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª >= active_start çš„ beat åˆ‡ç‚¹
    grid_in = grid[(grid >= active_start) & (grid <= active_end)]
    if grid_in.size == 0:
        # fallbackï¼šè‡³å°‘è¾“å‡ºä¸€ä¸ªæ•´ä½“åˆ‡ç‰‡
        return [{
            "id": 1,
            "start": round(active_start, 3),
            "end": round(active_end, 3),
            "duration": round(active_end - active_start, 3),
            "label": "Slice 1"
        }]

    first_cut = float(grid_in[0])
    # å¦‚æœ active_start åˆ° first_cut å¤ªçŸ­ï¼ˆå¼±èµ·/å™ªå£°ï¼‰ï¼Œç›´æ¥ä» first_cut å¼€å§‹
    if (first_cut - active_start) < 0.5 * period:
        cuts.append(first_cut)
    else:
        cuts.append(active_start)
        cuts.append(first_cut)

    # ä¸­é—´åˆ‡ç‚¹
    for t in grid_in[1:]:
        cuts.append(float(t))

    # æœ«å°¾ï¼šå¦‚æœæœ€åæ®‹ä½™å¤ªçŸ­ï¼Œå°±åˆå¹¶åˆ°ä¸Šä¸€æ‹ï¼ˆé¿å…å¾ˆçŸ­çš„æœ€åä¸€ä¸ª sliceï¼‰
    if len(cuts) >= 2:
        rem = active_end - cuts[-1]
        if rem < 0.25 * period:
            # åˆå¹¶ï¼šç§»é™¤æœ€åä¸€ä¸ª beat åˆ‡ç‚¹ï¼Œè®©æœ€åä¸€æ®µæ›´é•¿æ›´è‡ªç„¶
            if len(cuts) >= 3:
                cuts.pop()
    cuts.append(active_end)

    # 6) æ¯ä¸ªåˆ‡ç‚¹å¾®è°ƒåˆ°è¿‡é›¶ç‚¹ï¼ˆå‡å°‘ç‚¹å‡»éŸ³ï¼‰
    refined_cuts = []
    for t in cuts:
        refined_cuts.append(refine_time_to_zero_crossing(y, sr, t, window_ms=8))
    refined_cuts = np.array(refined_cuts, dtype=float)
    refined_cuts = np.sort(np.unique(refined_cuts))

    # 7) ç”Ÿæˆ slices
    slices = []
    sid = 1
    for i in range(len(refined_cuts) - 1):
        s = float(refined_cuts[i])
        e = float(refined_cuts[i + 1])
        if e - s < 0.03:
            continue
        slices.append({
            "id": sid,
            "start": round(s, 3),
            "end": round(e, 3),
            "duration": round(e - s, 3),
            "label": f"Slice {sid}"
        })
        sid += 1
    return slices

def generate_fcpxml(slices, filename, sample_rate, total_duration):
    xml_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE fcpxml>
<fcpxml version="1.8">
    <resources>
        <asset id="r1" name="{filename}" src="file://localhost/{filename}" start="0s" duration="{total_duration}s" hasAudio="1" hasVideo="0" />
    </resources>
    <library>
        <event name="Beat Slices">
            <project name="Beat Slices Project">
                <sequence duration="{total_duration}s" format="r1" tcStart="0s" tcFormat="NDF" audioLayout="stereo" audioRate="{sample_rate}">
                    <spine>
"""
    for s in slices:
        start = s['start']
        dur = s['duration']
        label = s['label']
        xml_content += f"""                        <asset-clip name="{label}" ref="r1" offset="{start}s" duration="{dur}s" start="{start}s" audioRole="dialogue">
                            <marker start="{start}s" duration="0.01s" value="{label}"/>
                            <adjust-volume amount="0dB">
                                <param name="level" key="Level" value="0dB">
                                    <keyframe time="0s" value="0dB"/>
                                </param>
                            </adjust-volume>
                        </asset-clip>\n"""

    xml_content += """                    </spine>
                </sequence>
            </project>
        </event>
    </library>
</fcpxml>"""
    return xml_content

def plot_interactive_waveform(y, sr, beat_times):
    # Downsample for performance (target ~10k points max)
    step = max(1, len(y) // 10000)
    y_subs = y[::step]
    x_subs = np.arange(len(y_subs)) * step / sr
    
    fig = go.Figure()
    
    # Waveform
    fig.add_trace(go.Scatter(
        x=x_subs, y=y_subs,
        mode='lines',
        name='Waveform',
        line=dict(color='#00CC96', width=1),
        hoverinfo='x+y'
    ))
    
    # Beat markers (using shapes is faster than individual traces for many lines)
    shapes = []
    # Limit visible vertical lines to avoid browser crash on very long tracks
    # We display lines for the first 500 beats max, or maybe all if not huge
    display_beats = beat_times if len(beat_times) < 1000 else beat_times[::2]
    
    for t in display_beats:
        shapes.append(dict(
            type="line",
            x0=t, x1=t,
            y0=0, y1=1,
            yref="paper",
            line=dict(color="#EF553B", width=1, dash="dot"),
            opacity=0.5
        ))

    # Add highlight for the last slice (Outro/Tail)
    total_duration = len(y) / sr
    if len(beat_times) > 0:
        last_beat = beat_times[-1]
        if last_beat < total_duration:
            # Add a subtle red background for the last segment
            shapes.append(dict(
                type="rect",
                x0=last_beat, 
                x1=total_duration,
                y0=0, y1=1,
                yref="paper",
                fillcolor="#EF553B", 
                opacity=0.1, 
                line_width=0,
            ))
            # Add a text label
            fig.add_annotation(
                x=(last_beat + total_duration) / 2,
                y=0.95,
                yref="paper",
                text="End Tail",
                showarrow=False,
                font=dict(color="#EF553B", size=10)
            )

    fig.update_layout(
        title="Interactive Waveform (Zoom/Pan enabled)",
        xaxis_title="Time (seconds)",
        yaxis_title="Amplitude",
        template="plotly_dark",
        margin=dict(l=20, r=20, t=40, b=20),
        height=350,
        dragmode='pan',
        shapes=shapes,
        xaxis=dict(
            rangeslider=dict(visible=True),
            range=[0, min(len(y)/sr, 20)] # Start zoomed in on first 20s
        )
    )
    return fig

# --- Sidebar Upload ---
with st.sidebar:
    st.header("Upload Audio")
    uploaded_file = st.file_uploader("Music File", type=["mp3", "wav"])
    
    if uploaded_file:
        if st.button("ğŸš€ Analyze & Slice", type="primary", use_container_width=True):
            # Progress Bar Setup
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                status_text.text("Initializing audio engine...")
                progress_bar.progress(5)
                
                suffix = ".mp3" if uploaded_file.name.endswith(".mp3") else ".wav"
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tfile:
                    tfile.write(uploaded_file.read())
                    tpath = tfile.name
                
                # 1. Initialize Remixer (Standard Analysis)
                status_text.text("Analyzing rhythm structure (BPM)...")
                progress_bar.progress(50)
                
                remixer = AudioRemixer(tpath)
                remixer.analyze() 
                st.session_state.remixer = remixer
                
                # Generate Slices
                # NEW: Passing y and sr for refinement
                bpm_info = estimate_bpm_best(remixer.y, remixer.sr, bpm_min=75.0, bpm_max=200.0)
                st.session_state.bpm_info = bpm_info
                slices = get_beat_slices(
                    remixer.y,
                    remixer.sr,
                    remixer.beat_times,
                    remixer.duration,
                    bpm_override=bpm_info.get("bpm", None) if isinstance(bpm_info, dict) else None
                )
                st.session_state.beat_slices = slices
                
                progress_bar.progress(100)
                
                # SKIP DRUM EXTRACTION (Optimization since tab is removed)
                # status_text.text("Separating stems for drum processing...")
                # ...
                
                os.remove(tpath)
                status_text.empty()
                progress_bar.empty()
                st.success("Analysis Complete!")
                
            except Exception as e:
                st.error(f"An error occurred: {e}")
                if 'tpath' in locals() and os.path.exists(tpath):
                    os.remove(tpath)

# --- Main Content ---
if not uploaded_file:
    st.info("ğŸ‘‹ Upload an audio file to start.")
    
elif st.session_state.remixer:
    remixer = st.session_state.remixer
    
    # --- Info Header ---
    st.divider()
    
    # Calculate Audio Statsï¼ˆé¿å…â€œè‡ªæ´½é™·é˜±â€ï¼šBPM ä¸å†æ ¹æ® slices åæ¨ï¼Œè€Œæ˜¯ç‹¬ç«‹ä¼°è®¡åå†é©±åŠ¨åˆ‡ç‰‡ï¼‰
    slice_starts = [s.get('start') for s in (st.session_state.beat_slices or []) if isinstance(s, dict)]
    bpm_from_slices = estimate_bpm_from_times(slice_starts) if len(slice_starts) > 2 else 0.0
    bpm_lib = estimate_bpm_librosa(remixer.y, remixer.sr)

    bpm_info = st.session_state.bpm_info
    if not isinstance(bpm_info, dict) or bpm_info.get("bpm", 0) <= 0:
        bpm_info = estimate_bpm_best(remixer.y, remixer.sr, bpm_min=75.0, bpm_max=200.0)
        st.session_state.bpm_info = bpm_info

    est_bpm = float(bpm_info.get("bpm", 0.0) or 0.0)
    
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("BPMï¼ˆä¼°è®¡ï¼‰", f"{est_bpm:.1f}")
    c2.metric("æ€»æ—¶é•¿", f"{remixer.duration:.2f}s")
    c3.metric("åˆ‡ç‰‡æ•°é‡", f"{len(st.session_state.beat_slices) if st.session_state.beat_slices else 0}")
    c4.metric("é‡‡æ ·ç‡", f"{remixer.sr} Hz")

    # æ‰‹åŠ¨ BPM ä¿®æ­£
    with st.expander("ğŸ› ï¸ æ‰‹åŠ¨ä¿®æ­£ BPM / é‡æ–°åˆ‡ç‰‡", expanded=False):
        manual_bpm = st.number_input(
            "è¾“å…¥ BPM æ•°å€¼ (ä¿®æ”¹åå°†å¼ºåˆ¶é‡ç®—åˆ‡ç‰‡)", 
            value=est_bpm, 
            min_value=10.0, 
            max_value=300.0, 
            step=0.1,
            format="%.1f"
        )
        
        if st.button("ğŸ”„ æŒ‰æ­¤ BPM é‡æ–°åˆ‡ç‰‡", use_container_width=True):
            with st.spinner(f"æ­£åœ¨æŒ‰ BPM {manual_bpm} é‡æ–°ç”Ÿæˆåˆ‡ç‰‡..."):
                # Update session BPM info to force the override
                st.session_state.bpm_info = {"bpm": manual_bpm, "confidence": 1.0, "base_bpm": manual_bpm, "candidates": []}
                # Re-run slicing
                slices = get_beat_slices(
                    remixer.y,
                    remixer.sr,
                    remixer.beat_times,
                    remixer.duration,
                    bpm_override=manual_bpm
                )
                st.session_state.beat_slices = slices
                st.rerun()

    st.divider()
    
    # --- BPM Slicer Visualization (No Tabs) ---
    st.subheader("BPM-Based Slicing Visualization")
    
    # Interactive Plotly Waveform
    # NOTE: beat_times passed to plot should ideally be the REFINED start times from slices
    # to match what the user sees in the table.
    # Extract start times from slices for plotting consistency
    refined_starts = [s['start'] for s in st.session_state.beat_slices if s['start'] < remixer.duration]
    
    fig_interactive = plot_interactive_waveform(remixer.y, remixer.sr, refined_starts)
    st.plotly_chart(fig_interactive, use_container_width=True)
    
    # --- Slice Previews ---
    st.divider()
    st.subheader("ğŸµ Slice Previews (First 10)")
    
    if st.session_state.beat_slices:
        preview_slices = st.session_state.beat_slices[:10]
        
        # Display in rows of 5
        rows = [preview_slices[i:i+5] for i in range(0, len(preview_slices), 5)]
        
        for row_items in rows:
            cols = st.columns(5)
            for idx, s in enumerate(row_items):
                with cols[idx]:
                    st.markdown(f"**{s['label']}**")
                    st.caption(f"{s['start']:.2f}s - {s['end']:.2f}s")
                
                    # Extract audio chunk
                    start_samp = int(s['start'] * remixer.sr)
                    end_samp = int(s['end'] * remixer.sr)
                    end_samp = min(end_samp, len(remixer.y))
                    
                    if start_samp < end_samp:
                        chunk = remixer.y[start_samp:end_samp]
                        # Normalize for preview
                        mx = np.max(np.abs(chunk))
                        if mx > 0: chunk = chunk / mx * 0.95
                        # Short fades to avoid clicks
                        chunk = apply_short_fade(chunk, remixer.sr, fade_ms=5)
                        
                        buf = io.BytesIO()
                        sf.write(buf, chunk, remixer.sr, format='WAV')
                        st.audio(buf.getvalue(), format='audio/wav')
                    
            # Spacer between rows
            st.write("")
    
    st.divider()

    # Download Section
    st.markdown("### Export All Slices")
    col_d1, col_d2, col_d3 = st.columns(3)
    
    if st.session_state.beat_slices:
        slices_data = st.session_state.beat_slices
        df_slices = pd.DataFrame(slices_data)
        
        # JSON
        json_str = json.dumps(slices_data, indent=2)
        col_d1.download_button(
            label="ğŸ“¥ Download JSON",
            data=json_str,
            file_name="slices.json",
            mime="application/json"
        )
        
        # Excel (with Fallback)
        excel_buffer = io.BytesIO()
        try:
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                df_slices.to_excel(writer, index=False, sheet_name='Slices')
            col_d2.download_button(
                label="ğŸ“¥ Download Excel",
                data=excel_buffer.getvalue(),
                file_name="slices.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        except Exception as e:
            # Fallback CSV
            csv_data = df_slices.to_csv(index=False).encode('utf-8')
            col_d2.download_button(
                label="ğŸ“¥ Download CSV (Excel Error)",
                data=csv_data,
                file_name="slices.csv",
                mime="text/csv"
            )
        
        # FCPXML
        fcpxml_str = generate_fcpxml(slices_data, uploaded_file.name, int(remixer.sr), remixer.duration)
        col_d3.download_button(
            label="ğŸ“¥ Download XML (.xml)",
            data=fcpxml_str,
            file_name="slices.xml",
            mime="application/xml",
            help="è¿™æ˜¯ FCPXML å†…å®¹ï¼ˆå¯å¯¼å…¥å‰ªè¾‘è½¯ä»¶ï¼‰ï¼Œä»…å°†æ‰©å±•åä¿å­˜ä¸º .xmlã€‚"
        )
        
        with st.expander("View Slice Data Table"):
            st.dataframe(df_slices, use_container_width=True)
